{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c69f11",
   "metadata": {},
   "source": [
    "# DSU ChatBot Internal Project Work\n",
    "## Installing Packages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb15d9-6b7a-400b-925f-c434bc4b95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain\n",
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e287909-50ed-4092-aad9-9a4d9bd42f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = 'REDACTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7722e486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "UCLA Data Science Union\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Skip to Content\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Clients\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        About\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Team\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Projects\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Contact\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Past Work\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Apply\n",
      "                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open Menu\n",
      "Close Menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Clients\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        About\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Team\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# lets make sure this works with the dsu loading page\n",
    "loader = WebBaseLoader(\"https://datascienceunion.com\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550bf01-115b-4b6f-afdb-dc94184319f9",
   "metadata": {},
   "source": [
    "# Importing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7cfac-b372-4189-a4e1-d59136110b06",
   "metadata": {},
   "source": [
    "Here we will import all of the pages in the DSU Website. We already loaded the landing page earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d02338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCLA Data Science Union\n",
      "Cultivating the next generation of data scientists\n",
      "    Meet Our Team\n",
      "  \n",
      "Who We Are\n",
      "The Data Science Union is a 501(c)(3) non-profit student-led organization founded in March 2019. Our goal is to cultivate a thriving community of tight-knit data science professionals by bringing together those who share an interest in data science. The four components of our club — our self-designed curriculum, real-world projects, professional development opportunities, and focus on community — prepare members for futures in data science.\n",
      "    Learn More\n",
      "  \n",
      "70+\n",
      "active members\n",
      "20+\n",
      "internal projects\n",
      "10+\n",
      "client projects\n",
      "15+\n",
      "companies represented\n",
      "Where We Work\n",
      "© 2024 Data Science Union. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Get the raw text\n",
    "raw_text_hp = docs[0].page_content\n",
    "\n",
    "# Replace multiple newlines with a single newline\n",
    "clean_text_hp = re.sub(r'\\n+', '\\n', raw_text_hp).strip()\n",
    "\n",
    "lines = clean_text_hp.split(\"\\n\")  # Split text into lines\n",
    "\n",
    "# Keep the first line\n",
    "filtered_lines = [lines[0]]\n",
    "\n",
    "# Find the start of the desired section\n",
    "start_index = next(i for i, line in enumerate(lines) if line.startswith(\"Cultivating\"))\n",
    "\n",
    "# Append the remaining lines starting from \"Why Us?\"\n",
    "filtered_lines.extend(lines[start_index:])\n",
    "\n",
    "# Join the filtered lines back into a string\n",
    "cleaned_text_homepage = \"\\n\".join(filtered_lines)\n",
    "\n",
    "print(cleaned_text_homepage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe55143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Clients — UCLA Data Science Union\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Skip to Content\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Clients\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        About\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Team\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Projects\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Contact\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Past Work\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Apply\n",
      "                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open Menu\n",
      "Close Menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Clients\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        About\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Team\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Projects\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Contact\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "        Past Work\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Apply\n",
      "                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open Menu\n",
      "Close Menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  Clients\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  About\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  Team\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  Projects\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  Contact\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  Past Work\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Apply\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "loader2 = WebBaseLoader(\"https://datascienceunion.com/clients\")\n",
    "docs2 = loader2.load()\n",
    "print(docs2[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5345dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients — UCLA Data Science Union\n",
      "Why Us?\n",
      "With our proprietary data science curriculum, we train our members to become experts in modern data science technologies. We construct each team to provide clients with experts in software engineering, machine learning, and data strategy. We have provided services in AI, analytics engineering, predictive modeling, software development, and  market research. Our members go on to work at top companies such as Microsoft, Netflix, Amazon, Figma, and Notion, offering clients the opportunity to work with future leaders in technology.\n",
      "The Data Science Union has meaningfully improved our ability to carry out our mission. The quality of their work and the responsiveness of their team made it a joy to work with them, and after several engagements with DSU, FoodFinder was able to turn the insights they gave us into a roadmap on how to better serve hungry families across America in a post-COVID world.\n",
      "Jack Griffin (Founder/CEO of FoodFinder)\n",
      "\"I had a fantastic experience working with The Data Science Union. Study Break needed help in designing, conducting, and analyzing results of a student survey to learn more about the college club fundraising landscape. The DSU team was not only knowledgeable, but highly responsive to any questions and concerns. The invaluable insights from the survey have provided Study Break with a clear action plan to better address student needs. I highly endorse DSU for their excellent service, expertise, and professionalism.\"\n",
      "Anthony Chao (Founder/CEO of Study Break)\n",
      "Our Services\n",
      "            Data Visualization\n",
      "          \n",
      "We transform complex data into actionable insights with modern dashboarding suites, including Tableau, PowerBI, Looker, and more.\n",
      "            Data Integration & Management\n",
      "          \n",
      "We streamline data collection, storage, and retrieval processes to ensure data integrity and accessibility.\n",
      "            Predictive Modeling\n",
      "          \n",
      "Utilizing machine learning algorithms to forecast trends and behaviors, our teams help clients anticipate market changes and customer needs.\n",
      "            Custom Workshops and Training\n",
      "          \n",
      "Equip your team with the skills needed to leverage data effectively through customized workshops led by our knowledgeable members.\n",
      "Past Clients\n",
      "© 2024 Data Science Union. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "# we dont need that many newlines, let's cut that one down\n",
    "import re\n",
    "\n",
    "# Get the raw text\n",
    "raw_text = docs2[0].page_content\n",
    "\n",
    "# Replace multiple newlines with a single newline\n",
    "clean_text = re.sub(r'\\n+', '\\n', raw_text).strip()\n",
    "\n",
    "lines = clean_text.split(\"\\n\")  # Split text into lines\n",
    "\n",
    "# Keep the first line\n",
    "filtered_lines = [lines[0]]\n",
    "\n",
    "# Find the start of the desired section\n",
    "start_index = next(i for i, line in enumerate(lines) if line.startswith(\"Why Us?\"))\n",
    "\n",
    "# Append the remaining lines starting from \"Why Us?\"\n",
    "filtered_lines.extend(lines[start_index:])\n",
    "\n",
    "# Join the filtered lines back into a string\n",
    "cleaned_text_clients = \"\\n\".join(filtered_lines)\n",
    "\n",
    "print(cleaned_text_clients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f74e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About — UCLA Data Science Union\n",
      "We’re a community\n",
      "Our Core Components\n",
      "curriculum\n",
      "projects\n",
      "professional development\n",
      "community\n",
      "Curriculum\n",
      "Our self-designed curriculum supplements and enhances UCLA coursework by giving new members the necessary skills and knowledge necessary to complete end-to-end data science projects. The first part of our two-quarter curriculum covers everything from basic programming in Python to building advanced machine learning models. No programming experience is required and our members come from a wide array of majors, backgrounds, and skill sets. The second part of our curriculum allows members to undertake independent, self-guided projects which relate data science to topics of personal interest.\n",
      "Projects\n",
      "After completing DSU’s curriculum, every member has the opportunity to work on either an internal or client project each quarter. Members on internal projects work collaboratively with other members throughout the quarter to answer questions using techniques ranging from classical machine learning to deep learning. Members on client projects partner with companies to help answer their data-driven questions, providing critical exposure to data science in industry.\n",
      "Professional Development\n",
      "Since our overarching goal is to serve as a pipeline toward careers in data science, we offer many opportunities for professional development. These include networking events with working professionals, industry panels with data science experts, behavioral interview preparation workshops, and professional connections with our alumni and client network.\n",
      "Community\n",
      "In addition to help members grow professionally, we also place a large emphasis on personal growth. At DSU, we seek to cultivate an environment in which members can form meaningful and long-lasting friendships with each other. Every year, we host a number of events for our members, including club socials and our winter and spring retreats.\n",
      "© 2024 Data Science Union. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "loader3 = WebBaseLoader(\"https://datascienceunion.com/about\")\n",
    "docs3 = loader3.load()\n",
    "#print(docs3[0].page_content)\n",
    "\n",
    "# clean it up\n",
    "raw_text3 = docs3[0].page_content\n",
    "\n",
    "# Replace multiple newlines with a single newline\n",
    "clean_text3 = re.sub(r'\\n+', '\\n', raw_text3).strip()\n",
    "\n",
    "# Print the cleaned content\n",
    "#print(clean_text3[:3000])\n",
    "\n",
    "lines = clean_text3.split(\"\\n\")  # Split text into lines\n",
    "\n",
    "# Keep the first line\n",
    "filtered_lines = [lines[0]]\n",
    "\n",
    "# Find the start of the desired section\n",
    "start_index = next(i for i, line in enumerate(lines) if line.startswith(\"We’re a community\"))\n",
    "\n",
    "# Append the remaining lines starting from \"We’re a community\"\n",
    "filtered_lines.extend(lines[start_index:])\n",
    "\n",
    "# Join the filtered lines back into a string\n",
    "cleaned_text_about = \"\\n\".join(filtered_lines)\n",
    "\n",
    "print(cleaned_text_about)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d85a22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team — UCLA Data Science Union\n",
      "Executive Board\n",
      "Ved Phadke\n",
      "President\n",
      "Belle Ho\n",
      "External Vice President\n",
      "Justin Gong\n",
      "Internal Vice President\n",
      "Jacob Bianchi\n",
      "Director of Data Science Training\n",
      "Kaylin Chung\n",
      "Director of Data Science Training\n",
      "Caleb Tran\n",
      "Director of Finance\n",
      "Sonia Kang\n",
      "Director of Marketing\n",
      "Riley Leong\n",
      "Director of Client Relations\n",
      "Danelle Lizardo\n",
      "Director of Professional Development\n",
      "Hannah Um\n",
      "Director of Projects\n",
      "Charlie Hoose\n",
      "Director of Membership\n",
      "Daniel Mendelevitch\n",
      "Director of Research\n",
      "Madeline Kim\n",
      "Executive Advisor\n",
      "Chloe Li\n",
      "Executive Advisor\n",
      "Members\n",
      "Aashima Khanna\n",
      "Class of 2027\n",
      "Aayushi Choudhary\n",
      "Class of 2025\n",
      "Aditya Murthy\n",
      "Class of 2026\n",
      "Ajeeth Iyer\n",
      "Class of 2026\n",
      "Alexandria Hunt\n",
      "Class of 2025\n",
      "Anika Lala\n",
      "Class of 2027\n",
      "Anthony Chen\n",
      "Class of 2026\n",
      "Arjun Asudani\n",
      "Class of 2027\n",
      "Aryan Sunkersett\n",
      "Class of 2026\n",
      "Athena Mo\n",
      "Class of 2027\n",
      "Ayushi Kadakia\n",
      "Class of 2027\n",
      "Benjamin Gelman\n",
      "Class of 2027\n",
      "Bree Zhiyi Chen\n",
      "Class of 2027\n",
      "Brook Chuang\n",
      "Class of 2026\n",
      "Camden Weber\n",
      "Class of 2025\n",
      "Casey Tattersall\n",
      "Class of 2024\n",
      "Charlie Moreno\n",
      "Class of 2025\n",
      "Cheyenne Lu\n",
      "Class of 2027\n",
      "Clare Kim\n",
      "Class of 2027\n",
      "Clement Truong\n",
      "Class of 2027\n",
      "Coco Baek\n",
      "Class of 2026\n",
      "Connie Gao\n",
      "Class of 2027\n",
      "Cynthia Du\n",
      "Class of 2026\n",
      "David Oplatka\n",
      "Class of 2026\n",
      "Defne Tanyildiz\n",
      "Class of 2026\n",
      "Dhwani Beesanahalli\n",
      "Class of 2028\n",
      "Dylan Winward\n",
      "Class of 2026\n",
      "Emil Goubasarian\n",
      "Class of 2025\n",
      "Emilio Dulay\n",
      "Class of 2026\n",
      "Emma Wu\n",
      "Class of 2027\n",
      "Eric Huang\n",
      "Class of 2025\n",
      "Gregor MacDonald\n",
      "Class of 2026\n",
      "Harry Tong\n",
      "Class of 2026\n",
      "Isaac Blender\n",
      "Class of 2025\n",
      "Janet Yu\n",
      "Class of 2026\n",
      "Jerome Teo\n",
      "Class of 2026\n",
      "Jessica Guan\n",
      "Class of 2027\n",
      "Jessup Byun\n",
      "Class of 2027\n",
      "Jonah Jung\n",
      "Class of 2025\n",
      "Justin Nguyen\n",
      "Class of 2028\n",
      "Kaitlyn Baughman\n",
      "Class of 2027\n",
      "Karl Wang\n",
      "Class of 2027\n",
      "Kayla Hamakawa\n",
      "Class of 2027\n",
      "Kevin Hamakawa\n",
      "Class of 2025\n",
      "Kiara Cueva\n",
      "Class of 2026\n",
      "Kimberly Cui\n",
      "Class of 2027\n",
      "Lara Papasian\n",
      "Class of 2025\n",
      "Leah Shin\n",
      "Class of 2027\n",
      "Leo Cardozo\n",
      "Class of 2026\n",
      "Loretta Hu\n",
      "Class of 2025\n",
      "Madelaine Leitman\n",
      "Class of 2025\n",
      "Madelyn Yip\n",
      "Class of 2027\n",
      "Maia Smolyanov\n",
      "Class of 2025\n",
      "Megan Nguyen\n",
      "Class of 2027\n",
      "Miles Garofola-Lam\n",
      "Class of 2025\n",
      "Millie Huang\n",
      "Class of 2026\n",
      "Nathan Trux\n",
      "Class of 2025\n",
      "Nathanael Nam\n",
      "Class of 2025\n",
      "Nicole Yamachika\n",
      "Class of 2025\n",
      "Niket Patel\n",
      "Class of 2025\n",
      "Om Phadke\n",
      "Class of 2028\n",
      "Parth Doshi\n",
      "Class of 2025\n",
      "Ria Kundu\n",
      "Class of 2027\n",
      "Rithwik Narendra\n",
      "Class of 2026\n",
      "Riya Bhatla\n",
      "Class of 2026\n",
      "Roy Cheng\n",
      "Class of 2027\n",
      "Rushil Shah\n",
      "Class of 2027\n",
      "Samantha Alejandre\n",
      "Class of 2025\n",
      "Samuel Perrott\n",
      "Class of 2026\n",
      "Sanskriti Jain\n",
      "Class of 2028\n",
      "Shan Kunzru\n",
      "Class of 2026\n",
      "Shravani Chiddarwar\n",
      "Class of 2027\n",
      "Sneha Agarwal\n",
      "Class of 2027\n",
      "Song Chen\n",
      "Class of 2026\n",
      "Syed Islam\n",
      "Class of 2027\n",
      "Tanmay Desai\n",
      "Class of 2025\n",
      "Tony He\n",
      "Class of 2026\n",
      "Vivian Lee\n",
      "Class of 2025\n",
      "Zahra Umar\n",
      "Class of 2026\n",
      "© 2024 Data Science Union. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "loader4 = WebBaseLoader(\"https://datascienceunion.com/team\")\n",
    "docs4 = loader4.load()\n",
    "#print(docs3[0].page_content)\n",
    "\n",
    "# clean it up\n",
    "raw_text4 = docs4[0].page_content\n",
    "\n",
    "# Replace multiple newlines with a single newline\n",
    "clean_text4 = re.sub(r'\\n+', '\\n', raw_text4).strip()\n",
    "\n",
    "# Print the cleaned content\n",
    "#print(clean_text4[:4000])\n",
    "\n",
    "lines = clean_text4.split(\"\\n\")  # Split text into lines\n",
    "\n",
    "# Keep the first line\n",
    "filtered_lines = [lines[0]]\n",
    "\n",
    "# Find the start of the desired section\n",
    "start_index = next(i for i, line in enumerate(lines) if line.startswith(\"Executive Board\"))\n",
    "\n",
    "# Append the remaining lines starting from \"We’re a community\"\n",
    "filtered_lines.extend(lines[start_index:])\n",
    "\n",
    "# Join the filtered lines back into a string\n",
    "cleaned_text_team = \"\\n\".join(filtered_lines)\n",
    "\n",
    "print(cleaned_text_team)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c90ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects — UCLA Data Science Union\n",
      "Featured Internal Projects\n",
      "Internal Projects are pitched by club members and focus on using various data science techniques to creatively explore a topic of interest. Check out some of our previous work below or on our medium or github.\n",
      "    Social Intelligence: Emotion Prediction in Videos\n",
      "  \n",
      "    News Generation\n",
      "  \n",
      "    Text Summarization\n",
      "  \n",
      "    Enhancing Spotify Playlists using their Audio Features with Classical and Deep Learning Methods\n",
      "  \n",
      "    Predicting Plane Ticket Prices\n",
      "  \n",
      "    Healthy Communities\n",
      "  \n",
      "© 2024 Data Science Union. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "loader5 = WebBaseLoader(\"https://datascienceunion.com/projects\")\n",
    "docs5 = loader5.load()\n",
    "#print(docs3[0].page_content)\n",
    "\n",
    "# clean it up\n",
    "raw_text5 = docs5[0].page_content\n",
    "\n",
    "# Replace multiple newlines with a single newline\n",
    "clean_text5 = re.sub(r'\\n+', '\\n', raw_text5).strip()\n",
    "\n",
    "# Print the cleaned content\n",
    "#print(clean_text5[:3000])\n",
    "\n",
    "lines = clean_text5.split(\"\\n\")  # Split text into lines\n",
    "\n",
    "# Keep the first line\n",
    "filtered_lines = [lines[0]]\n",
    "\n",
    "# Find the start of the desired section\n",
    "start_index = next(i for i, line in enumerate(lines) if line.startswith(\"Featured Internal Projects\"))\n",
    "\n",
    "# Append the remaining lines starting from \"We’re a community\"\n",
    "filtered_lines.extend(lines[start_index:])\n",
    "\n",
    "# Join the filtered lines back into a string\n",
    "cleaned_text_projects = \"\\n\".join(filtered_lines)\n",
    "\n",
    "print(cleaned_text_projects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4600198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact — UCLA Data Science Union\n",
      "Contact Us\n",
      "Are you a company looking to collaborate on a project with us? Are you a student with questions about what we do? Feel free to contact us and we will get back to you as soon as we can!\n",
      "For general inquiries, our email is ucladsu@gmail.com.\n",
      "              Name\n",
      "              \n",
      "            \n",
      "              Email\n",
      "              \n",
      "            \n",
      "              Subject\n",
      "              \n",
      "            \n",
      "              Message\n",
      "              \n",
      "            \n",
      "Thank you!\n",
      "© 2024 Data Science Union. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "loader6 = WebBaseLoader(\"https://datascienceunion.com/contact\")\n",
    "docs6 = loader6.load()\n",
    "#print(docs3[0].page_content)\n",
    "\n",
    "# clean it up\n",
    "raw_text6 = docs6[0].page_content\n",
    "\n",
    "# Replace multiple newlines with a single newline\n",
    "clean_text6 = re.sub(r'\\n+', '\\n', raw_text6).strip()\n",
    "\n",
    "# Print the cleaned content\n",
    "#print(clean_text6[:3000])\n",
    "\n",
    "lines = clean_text6.split(\"\\n\")  # Split text into lines\n",
    "\n",
    "# Keep the first line\n",
    "filtered_lines = [lines[0]]\n",
    "\n",
    "# Find the start of the desired section\n",
    "start_index = next(i for i, line in enumerate(lines) if line.startswith(\"Contact Us\"))\n",
    "\n",
    "# Append the remaining lines starting from \"We’re a community\"\n",
    "filtered_lines.extend(lines[start_index:])\n",
    "\n",
    "# Join the filtered lines back into a string\n",
    "cleaned_text_contact = \"\\n\".join(filtered_lines)\n",
    "\n",
    "print(cleaned_text_contact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56fbdb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply — UCLA Data Science Union\n",
      "Recruitment\n",
      "Our application for the fall recruitment cycle has closed. We’ll be recruiting again Spring 2025 and look forward to reading your application!\n",
      "FAQ\n",
      "            What does DSU look for in their applicants?\n",
      "          \n",
      "We look for students who are passionate about developing their  skills in data science alongside like-minded peers and seek to answer questions through data-driven methods. We love seeing how you are personally inspired by data science, and we want to hear about how DSU fits into that inspiration!\n",
      "            What skills is DSU expecting? Do I need to take certain classes to be prepared?\n",
      "          \n",
      "We recommend that applicants have some prior experience with data analysis or statistical programming, but this is not strictly necessary. Many of our members have little to no experience with programming or data science when they join. Our proprietary two-part curriculum is designed to get all new members up to speed and ready to contribute to projects!\n",
      "            How can I prepare for the interviews?\n",
      "          \n",
      "Be yourself, and bring your personality! Be ready to talk about who you are, your values, and your goals in both a group and individual setting!\n",
      "            Can I apply if I am an incoming freshman? Are there any major restrictions for applying?\n",
      "          \n",
      "We welcome all students, regardless of their year or academic background. Membership is only available to current UCLA students at this time.\n",
      "             I have experience doing data-related projects. Can I skip curriculum and directly join the project teams?\n",
      "          \n",
      "If you’re interested in skipping the curriculum track, let us know during your interview! We’ll follow up with you to determine whether you’re a good fit to directly join project teams.\n",
      "© 2024 Data Science Union. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "loader7 = WebBaseLoader(\"https://datascienceunion.com/apply\")\n",
    "docs7 = loader7.load()\n",
    "#print(docs3[0].page_content)\n",
    "\n",
    "# clean it up\n",
    "raw_text7 = docs7[0].page_content\n",
    "\n",
    "# Replace multiple newlines with a single newline\n",
    "clean_text7 = re.sub(r'\\n+', '\\n', raw_text7).strip()\n",
    "\n",
    "# Print the cleaned content\n",
    "#print(clean_text7[:3000])\n",
    "\n",
    "lines = clean_text7.split(\"\\n\")  # Split text into lines\n",
    "\n",
    "# Keep the first line\n",
    "filtered_lines = [lines[0]]\n",
    "\n",
    "# Find the start of the desired section\n",
    "start_index = next(i for i, line in enumerate(lines) if line.startswith(\"Recruitment\"))\n",
    "\n",
    "# Append the remaining lines starting from \"We’re a community\"\n",
    "filtered_lines.extend(lines[start_index:])\n",
    "\n",
    "# Join the filtered lines back into a string\n",
    "cleaned_text_apply = \"\\n\".join(filtered_lines)\n",
    "\n",
    "print(cleaned_text_apply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11549bb",
   "metadata": {},
   "source": [
    "We now have 7 different strings for each of the 7 sections in the website: homepage, clients, about, team, projects, contact, and apply! We can move on to splitting them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376e921",
   "metadata": {},
   "source": [
    "# Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dd693de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99860226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UCLA Data Science Union\\nCultivating the next generation of data scientists\\n    Meet Our Team\\n  \\nWho We Are',\n",
       " 'The Data Science Union is a 501(c)(3) non-profit student-led organization founded in March 2019. Our goal is to cultivate a thriving community of tight-knit data science professionals by bringing',\n",
       " 'tight-knit data science professionals by bringing together those who share an interest in data science. The four components of our club — our self-designed curriculum, real-world projects,',\n",
       " 'self-designed curriculum, real-world projects, professional development opportunities, and focus on community — prepare members for futures in data science.',\n",
       " 'Learn More\\n  \\n70+\\nactive members\\n20+\\ninternal projects\\n10+\\nclient projects\\n15+\\ncompanies represented\\nWhere We Work\\n© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Homepage tab\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "homepage_split = r_splitter.split_text(cleaned_text_homepage)\n",
    "homepage_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1f0f7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Clients — UCLA Data Science Union\\nWhy Us?',\n",
       " 'With our proprietary data science curriculum, we train our members to become experts in modern data science technologies. We construct each team to provide clients with experts in software',\n",
       " 'team to provide clients with experts in software engineering, machine learning, and data strategy. We have provided services in AI, analytics engineering, predictive modeling, software development,',\n",
       " 'predictive modeling, software development, and  market research. Our members go on to work at top companies such as Microsoft, Netflix, Amazon, Figma, and Notion, offering clients the opportunity to',\n",
       " 'and Notion, offering clients the opportunity to work with future leaders in technology.',\n",
       " 'The Data Science Union has meaningfully improved our ability to carry out our mission. The quality of their work and the responsiveness of their team made it a joy to work with them, and after',\n",
       " 'team made it a joy to work with them, and after several engagements with DSU, FoodFinder was able to turn the insights they gave us into a roadmap on how to better serve hungry families across',\n",
       " 'on how to better serve hungry families across America in a post-COVID world.',\n",
       " 'Jack Griffin (Founder/CEO of FoodFinder)',\n",
       " '\"I had a fantastic experience working with The Data Science Union. Study Break needed help in designing, conducting, and analyzing results of a student survey to learn more about the college club',\n",
       " 'survey to learn more about the college club fundraising landscape. The DSU team was not only knowledgeable, but highly responsive to any questions and concerns. The invaluable insights from the',\n",
       " 'and concerns. The invaluable insights from the survey have provided Study Break with a clear action plan to better address student needs. I highly endorse DSU for their excellent service, expertise,',\n",
       " 'DSU for their excellent service, expertise, and professionalism.\"',\n",
       " 'Anthony Chao (Founder/CEO of Study Break)\\nOur Services\\n            Data Visualization',\n",
       " 'Data Visualization\\n          \\nWe transform complex data into actionable insights with modern dashboarding suites, including Tableau, PowerBI, Looker, and more.',\n",
       " 'Data Integration & Management\\n          \\nWe streamline data collection, storage, and retrieval processes to ensure data integrity and accessibility.\\n            Predictive Modeling',\n",
       " 'Predictive Modeling\\n          \\nUtilizing machine learning algorithms to forecast trends and behaviors, our teams help clients anticipate market changes and customer needs.',\n",
       " 'Custom Workshops and Training\\n          \\nEquip your team with the skills needed to leverage data effectively through customized workshops led by our knowledgeable members.\\nPast Clients',\n",
       " 'Past Clients\\n© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clients tab\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "clients_split = r_splitter.split_text(cleaned_text_clients)\n",
    "clients_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea77f3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['About — UCLA Data Science Union\\nWe’re a community\\nOur Core Components\\ncurriculum\\nprojects\\nprofessional development\\ncommunity\\nCurriculum',\n",
       " 'Our self-designed curriculum supplements and enhances UCLA coursework by giving new members the necessary skills and knowledge necessary to complete end-to-end data science projects. The first part',\n",
       " 'end-to-end data science projects. The first part of our two-quarter curriculum covers everything from basic programming in Python to building advanced machine learning models. No programming',\n",
       " 'advanced machine learning models. No programming experience is required and our members come from a wide array of majors, backgrounds, and skill sets. The second part of our curriculum allows members',\n",
       " 'The second part of our curriculum allows members to undertake independent, self-guided projects which relate data science to topics of personal interest.',\n",
       " 'Projects',\n",
       " 'After completing DSU’s curriculum, every member has the opportunity to work on either an internal or client project each quarter. Members on internal projects work collaboratively with other members',\n",
       " 'projects work collaboratively with other members throughout the quarter to answer questions using techniques ranging from classical machine learning to deep learning. Members on client projects',\n",
       " 'to deep learning. Members on client projects partner with companies to help answer their data-driven questions, providing critical exposure to data science in industry.',\n",
       " 'Professional Development',\n",
       " 'Since our overarching goal is to serve as a pipeline toward careers in data science, we offer many opportunities for professional development. These include networking events with working',\n",
       " 'These include networking events with working professionals, industry panels with data science experts, behavioral interview preparation workshops, and professional connections with our alumni and',\n",
       " 'and professional connections with our alumni and client network.',\n",
       " 'Community',\n",
       " 'In addition to help members grow professionally, we also place a large emphasis on personal growth. At DSU, we seek to cultivate an environment in which members can form meaningful and long-lasting',\n",
       " 'members can form meaningful and long-lasting friendships with each other. Every year, we host a number of events for our members, including club socials and our winter and spring retreats.',\n",
       " '© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About tab\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "about_split = r_splitter.split_text(cleaned_text_about)\n",
    "about_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "239643e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Team — UCLA Data Science Union\\nExecutive Board',\n",
       " 'Executive Board\\nVed Phadke\\nPresident\\nBelle Ho',\n",
       " 'President\\nBelle Ho\\nExternal Vice President',\n",
       " 'Justin Gong\\nInternal Vice President\\nJacob Bianchi',\n",
       " 'Jacob Bianchi\\nDirector of Data Science Training',\n",
       " 'Kaylin Chung\\nDirector of Data Science Training',\n",
       " 'Caleb Tran\\nDirector of Finance\\nSonia Kang',\n",
       " 'Sonia Kang\\nDirector of Marketing\\nRiley Leong',\n",
       " 'Riley Leong\\nDirector of Client Relations',\n",
       " 'Danelle Lizardo',\n",
       " 'Director of Professional Development\\nHannah Um',\n",
       " 'Hannah Um\\nDirector of Projects\\nCharlie Hoose',\n",
       " 'Charlie Hoose\\nDirector of Membership',\n",
       " 'Daniel Mendelevitch\\nDirector of Research',\n",
       " 'Madeline Kim\\nExecutive Advisor\\nChloe Li',\n",
       " 'Chloe Li\\nExecutive Advisor\\nMembers\\nAashima Khanna',\n",
       " 'Aashima Khanna\\nClass of 2027\\nAayushi Choudhary',\n",
       " 'Aayushi Choudhary\\nClass of 2025\\nAditya Murthy',\n",
       " 'Aditya Murthy\\nClass of 2026\\nAjeeth Iyer',\n",
       " 'Ajeeth Iyer\\nClass of 2026\\nAlexandria Hunt',\n",
       " 'Alexandria Hunt\\nClass of 2025\\nAnika Lala',\n",
       " 'Anika Lala\\nClass of 2027\\nAnthony Chen',\n",
       " 'Anthony Chen\\nClass of 2026\\nArjun Asudani',\n",
       " 'Arjun Asudani\\nClass of 2027\\nAryan Sunkersett',\n",
       " 'Aryan Sunkersett\\nClass of 2026\\nAthena Mo',\n",
       " 'Athena Mo\\nClass of 2027\\nAyushi Kadakia',\n",
       " 'Ayushi Kadakia\\nClass of 2027\\nBenjamin Gelman',\n",
       " 'Benjamin Gelman\\nClass of 2027\\nBree Zhiyi Chen',\n",
       " 'Bree Zhiyi Chen\\nClass of 2027\\nBrook Chuang',\n",
       " 'Brook Chuang\\nClass of 2026\\nCamden Weber',\n",
       " 'Camden Weber\\nClass of 2025\\nCasey Tattersall',\n",
       " 'Casey Tattersall\\nClass of 2024\\nCharlie Moreno',\n",
       " 'Charlie Moreno\\nClass of 2025\\nCheyenne Lu',\n",
       " 'Cheyenne Lu\\nClass of 2027\\nClare Kim\\nClass of 2027',\n",
       " 'Class of 2027\\nClement Truong\\nClass of 2027',\n",
       " 'Class of 2027\\nCoco Baek\\nClass of 2026\\nConnie Gao',\n",
       " 'Connie Gao\\nClass of 2027\\nCynthia Du\\nClass of 2026',\n",
       " 'Class of 2026\\nDavid Oplatka\\nClass of 2026',\n",
       " 'Class of 2026\\nDefne Tanyildiz\\nClass of 2026',\n",
       " 'Class of 2026\\nDhwani Beesanahalli\\nClass of 2028',\n",
       " 'Class of 2028\\nDylan Winward\\nClass of 2026',\n",
       " 'Class of 2026\\nEmil Goubasarian\\nClass of 2025',\n",
       " 'Class of 2025\\nEmilio Dulay\\nClass of 2026\\nEmma Wu',\n",
       " 'Emma Wu\\nClass of 2027\\nEric Huang\\nClass of 2025',\n",
       " 'Class of 2025\\nGregor MacDonald\\nClass of 2026',\n",
       " 'Class of 2026\\nHarry Tong\\nClass of 2026',\n",
       " 'Class of 2026\\nIsaac Blender\\nClass of 2025',\n",
       " 'Class of 2025\\nJanet Yu\\nClass of 2026\\nJerome Teo',\n",
       " 'Jerome Teo\\nClass of 2026\\nJessica Guan',\n",
       " 'Jessica Guan\\nClass of 2027\\nJessup Byun',\n",
       " 'Jessup Byun\\nClass of 2027\\nJonah Jung',\n",
       " 'Jonah Jung\\nClass of 2025\\nJustin Nguyen',\n",
       " 'Justin Nguyen\\nClass of 2028\\nKaitlyn Baughman',\n",
       " 'Kaitlyn Baughman\\nClass of 2027\\nKarl Wang',\n",
       " 'Karl Wang\\nClass of 2027\\nKayla Hamakawa',\n",
       " 'Kayla Hamakawa\\nClass of 2027\\nKevin Hamakawa',\n",
       " 'Kevin Hamakawa\\nClass of 2025\\nKiara Cueva',\n",
       " 'Kiara Cueva\\nClass of 2026\\nKimberly Cui',\n",
       " 'Kimberly Cui\\nClass of 2027\\nLara Papasian',\n",
       " 'Lara Papasian\\nClass of 2025\\nLeah Shin',\n",
       " 'Leah Shin\\nClass of 2027\\nLeo Cardozo\\nClass of 2026',\n",
       " 'Class of 2026\\nLoretta Hu\\nClass of 2025',\n",
       " 'Class of 2025\\nMadelaine Leitman\\nClass of 2025',\n",
       " 'Class of 2025\\nMadelyn Yip\\nClass of 2027',\n",
       " 'Class of 2027\\nMaia Smolyanov\\nClass of 2025',\n",
       " 'Class of 2025\\nMegan Nguyen\\nClass of 2027',\n",
       " 'Class of 2027\\nMiles Garofola-Lam\\nClass of 2025',\n",
       " 'Class of 2025\\nMillie Huang\\nClass of 2026',\n",
       " 'Class of 2026\\nNathan Trux\\nClass of 2025',\n",
       " 'Class of 2025\\nNathanael Nam\\nClass of 2025',\n",
       " 'Class of 2025\\nNicole Yamachika\\nClass of 2025',\n",
       " 'Class of 2025\\nNiket Patel\\nClass of 2025\\nOm Phadke',\n",
       " 'Om Phadke\\nClass of 2028\\nParth Doshi\\nClass of 2025',\n",
       " 'Class of 2025\\nRia Kundu\\nClass of 2027',\n",
       " 'Class of 2027\\nRithwik Narendra\\nClass of 2026',\n",
       " 'Class of 2026\\nRiya Bhatla\\nClass of 2026\\nRoy Cheng',\n",
       " 'Roy Cheng\\nClass of 2027\\nRushil Shah\\nClass of 2027',\n",
       " 'Class of 2027\\nSamantha Alejandre\\nClass of 2025',\n",
       " 'Class of 2025\\nSamuel Perrott\\nClass of 2026',\n",
       " 'Class of 2026\\nSanskriti Jain\\nClass of 2028',\n",
       " 'Class of 2028\\nShan Kunzru\\nClass of 2026',\n",
       " 'Class of 2026\\nShravani Chiddarwar\\nClass of 2027',\n",
       " 'Class of 2027\\nSneha Agarwal\\nClass of 2027',\n",
       " 'Class of 2027\\nSong Chen\\nClass of 2026\\nSyed Islam',\n",
       " 'Syed Islam\\nClass of 2027\\nTanmay Desai',\n",
       " 'Tanmay Desai\\nClass of 2025\\nTony He\\nClass of 2026',\n",
       " 'Class of 2026\\nVivian Lee\\nClass of 2025\\nZahra Umar',\n",
       " 'Zahra Umar\\nClass of 2026',\n",
       " '© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Team tab\n",
    "chunk_size = 50\n",
    "chunk_overlap = 20\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "team_split = r_splitter.split_text(cleaned_text_team)\n",
    "team_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ea02e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Projects — UCLA Data Science Union\\nFeatured Internal Projects',\n",
       " 'Internal Projects are pitched by club members and focus on using various data science techniques to',\n",
       " 'data science techniques to creatively explore a topic of interest. Check out some of our previous',\n",
       " 'out some of our previous work below or on our medium or github.',\n",
       " 'Social Intelligence: Emotion Prediction in Videos\\n  \\n    News Generation',\n",
       " 'News Generation\\n  \\n    Text Summarization',\n",
       " 'Enhancing Spotify Playlists using their Audio Features with Classical and Deep Learning Methods',\n",
       " 'Predicting Plane Ticket Prices\\n  \\n    Healthy Communities',\n",
       " 'Healthy Communities\\n  \\n© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projects tab\n",
    "chunk_size = 100\n",
    "chunk_overlap = 30\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "projects_split = r_splitter.split_text(cleaned_text_projects)\n",
    "projects_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c0b9cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contact — UCLA Data Science Union\\nContact Us',\n",
       " 'Contact Us\\nAre you a company looking to collaborate on a project with us? Are you a student with questions about what we do? Feel free to contact us and we will get back to you as soon as we can!',\n",
       " 'For general inquiries, our email is ucladsu@gmail.com.\\n              Name\\n              \\n            \\n              Email\\n              \\n            \\n              Subject',\n",
       " 'Message\\n              \\n            \\nThank you!\\n© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contact tab\n",
    "chunk_size = 200\n",
    "chunk_overlap = 20\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "contact_split = r_splitter.split_text(cleaned_text_contact)\n",
    "contact_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68123fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apply — UCLA Data Science Union\\nRecruitment\\nOur application for the fall recruitment cycle has closed. We’ll be recruiting again Spring 2025 and look forward to reading your application!\\nFAQ\\n            What does DSU look for in their applicants?',\n",
       " 'We look for students who are passionate about developing their  skills in data science alongside like-minded peers and seek to answer questions through data-driven methods. We love seeing how you are personally inspired by data science, and we want to hear about how DSU fits into that inspiration!',\n",
       " 'What skills is DSU expecting? Do I need to take certain classes to be prepared?',\n",
       " 'We recommend that applicants have some prior experience with data analysis or statistical programming, but this is not strictly necessary. Many of our members have little to no experience with programming or data science when they join. Our proprietary two-part curriculum is designed to get all new',\n",
       " 'two-part curriculum is designed to get all new members up to speed and ready to contribute to projects!',\n",
       " 'How can I prepare for the interviews?\\n          \\nBe yourself, and bring your personality! Be ready to talk about who you are, your values, and your goals in both a group and individual setting!',\n",
       " 'Can I apply if I am an incoming freshman? Are there any major restrictions for applying?\\n          \\nWe welcome all students, regardless of their year or academic background. Membership is only available to current UCLA students at this time.',\n",
       " 'I have experience doing data-related projects. Can I skip curriculum and directly join the project teams?',\n",
       " 'If you’re interested in skipping the curriculum track, let us know during your interview! We’ll follow up with you to determine whether you’re a good fit to directly join project teams.\\n© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply tab\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "apply_split = r_splitter.split_text(cleaned_text_apply)\n",
    "apply_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ef58bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UCLA Data Science Union\\nCultivating the next generation of data scientists\\n    Meet Our Team\\n  \\nWho We Are',\n",
       " 'The Data Science Union is a 501(c)(3) non-profit student-led organization founded in March 2019. Our goal is to cultivate a thriving community of tight-knit data science professionals by bringing',\n",
       " 'tight-knit data science professionals by bringing together those who share an interest in data science. The four components of our club — our self-designed curriculum, real-world projects,',\n",
       " 'self-designed curriculum, real-world projects, professional development opportunities, and focus on community — prepare members for futures in data science.',\n",
       " 'Learn More\\n  \\n70+\\nactive members\\n20+\\ninternal projects\\n10+\\nclient projects\\n15+\\ncompanies represented\\nWhere We Work\\n© 2024 Data Science Union. All rights reserved.',\n",
       " 'Clients — UCLA Data Science Union\\nWhy Us?',\n",
       " 'With our proprietary data science curriculum, we train our members to become experts in modern data science technologies. We construct each team to provide clients with experts in software',\n",
       " 'team to provide clients with experts in software engineering, machine learning, and data strategy. We have provided services in AI, analytics engineering, predictive modeling, software development,',\n",
       " 'predictive modeling, software development, and  market research. Our members go on to work at top companies such as Microsoft, Netflix, Amazon, Figma, and Notion, offering clients the opportunity to',\n",
       " 'and Notion, offering clients the opportunity to work with future leaders in technology.',\n",
       " 'The Data Science Union has meaningfully improved our ability to carry out our mission. The quality of their work and the responsiveness of their team made it a joy to work with them, and after',\n",
       " 'team made it a joy to work with them, and after several engagements with DSU, FoodFinder was able to turn the insights they gave us into a roadmap on how to better serve hungry families across',\n",
       " 'on how to better serve hungry families across America in a post-COVID world.',\n",
       " 'Jack Griffin (Founder/CEO of FoodFinder)',\n",
       " '\"I had a fantastic experience working with The Data Science Union. Study Break needed help in designing, conducting, and analyzing results of a student survey to learn more about the college club',\n",
       " 'survey to learn more about the college club fundraising landscape. The DSU team was not only knowledgeable, but highly responsive to any questions and concerns. The invaluable insights from the',\n",
       " 'and concerns. The invaluable insights from the survey have provided Study Break with a clear action plan to better address student needs. I highly endorse DSU for their excellent service, expertise,',\n",
       " 'DSU for their excellent service, expertise, and professionalism.\"',\n",
       " 'Anthony Chao (Founder/CEO of Study Break)\\nOur Services\\n            Data Visualization',\n",
       " 'Data Visualization\\n          \\nWe transform complex data into actionable insights with modern dashboarding suites, including Tableau, PowerBI, Looker, and more.',\n",
       " 'Data Integration & Management\\n          \\nWe streamline data collection, storage, and retrieval processes to ensure data integrity and accessibility.\\n            Predictive Modeling',\n",
       " 'Predictive Modeling\\n          \\nUtilizing machine learning algorithms to forecast trends and behaviors, our teams help clients anticipate market changes and customer needs.',\n",
       " 'Custom Workshops and Training\\n          \\nEquip your team with the skills needed to leverage data effectively through customized workshops led by our knowledgeable members.\\nPast Clients',\n",
       " 'Past Clients\\n© 2024 Data Science Union. All rights reserved.',\n",
       " 'About — UCLA Data Science Union\\nWe’re a community\\nOur Core Components\\ncurriculum\\nprojects\\nprofessional development\\ncommunity\\nCurriculum',\n",
       " 'Our self-designed curriculum supplements and enhances UCLA coursework by giving new members the necessary skills and knowledge necessary to complete end-to-end data science projects. The first part',\n",
       " 'end-to-end data science projects. The first part of our two-quarter curriculum covers everything from basic programming in Python to building advanced machine learning models. No programming',\n",
       " 'advanced machine learning models. No programming experience is required and our members come from a wide array of majors, backgrounds, and skill sets. The second part of our curriculum allows members',\n",
       " 'The second part of our curriculum allows members to undertake independent, self-guided projects which relate data science to topics of personal interest.',\n",
       " 'Projects',\n",
       " 'After completing DSU’s curriculum, every member has the opportunity to work on either an internal or client project each quarter. Members on internal projects work collaboratively with other members',\n",
       " 'projects work collaboratively with other members throughout the quarter to answer questions using techniques ranging from classical machine learning to deep learning. Members on client projects',\n",
       " 'to deep learning. Members on client projects partner with companies to help answer their data-driven questions, providing critical exposure to data science in industry.',\n",
       " 'Professional Development',\n",
       " 'Since our overarching goal is to serve as a pipeline toward careers in data science, we offer many opportunities for professional development. These include networking events with working',\n",
       " 'These include networking events with working professionals, industry panels with data science experts, behavioral interview preparation workshops, and professional connections with our alumni and',\n",
       " 'and professional connections with our alumni and client network.',\n",
       " 'Community',\n",
       " 'In addition to help members grow professionally, we also place a large emphasis on personal growth. At DSU, we seek to cultivate an environment in which members can form meaningful and long-lasting',\n",
       " 'members can form meaningful and long-lasting friendships with each other. Every year, we host a number of events for our members, including club socials and our winter and spring retreats.',\n",
       " '© 2024 Data Science Union. All rights reserved.',\n",
       " 'Team — UCLA Data Science Union\\nExecutive Board',\n",
       " 'Executive Board\\nVed Phadke\\nPresident\\nBelle Ho',\n",
       " 'President\\nBelle Ho\\nExternal Vice President',\n",
       " 'Justin Gong\\nInternal Vice President\\nJacob Bianchi',\n",
       " 'Jacob Bianchi\\nDirector of Data Science Training',\n",
       " 'Kaylin Chung\\nDirector of Data Science Training',\n",
       " 'Caleb Tran\\nDirector of Finance\\nSonia Kang',\n",
       " 'Sonia Kang\\nDirector of Marketing\\nRiley Leong',\n",
       " 'Riley Leong\\nDirector of Client Relations',\n",
       " 'Danelle Lizardo',\n",
       " 'Director of Professional Development\\nHannah Um',\n",
       " 'Hannah Um\\nDirector of Projects\\nCharlie Hoose',\n",
       " 'Charlie Hoose\\nDirector of Membership',\n",
       " 'Daniel Mendelevitch\\nDirector of Research',\n",
       " 'Madeline Kim\\nExecutive Advisor\\nChloe Li',\n",
       " 'Chloe Li\\nExecutive Advisor\\nMembers\\nAashima Khanna',\n",
       " 'Aashima Khanna\\nClass of 2027\\nAayushi Choudhary',\n",
       " 'Aayushi Choudhary\\nClass of 2025\\nAditya Murthy',\n",
       " 'Aditya Murthy\\nClass of 2026\\nAjeeth Iyer',\n",
       " 'Ajeeth Iyer\\nClass of 2026\\nAlexandria Hunt',\n",
       " 'Alexandria Hunt\\nClass of 2025\\nAnika Lala',\n",
       " 'Anika Lala\\nClass of 2027\\nAnthony Chen',\n",
       " 'Anthony Chen\\nClass of 2026\\nArjun Asudani',\n",
       " 'Arjun Asudani\\nClass of 2027\\nAryan Sunkersett',\n",
       " 'Aryan Sunkersett\\nClass of 2026\\nAthena Mo',\n",
       " 'Athena Mo\\nClass of 2027\\nAyushi Kadakia',\n",
       " 'Ayushi Kadakia\\nClass of 2027\\nBenjamin Gelman',\n",
       " 'Benjamin Gelman\\nClass of 2027\\nBree Zhiyi Chen',\n",
       " 'Bree Zhiyi Chen\\nClass of 2027\\nBrook Chuang',\n",
       " 'Brook Chuang\\nClass of 2026\\nCamden Weber',\n",
       " 'Camden Weber\\nClass of 2025\\nCasey Tattersall',\n",
       " 'Casey Tattersall\\nClass of 2024\\nCharlie Moreno',\n",
       " 'Charlie Moreno\\nClass of 2025\\nCheyenne Lu',\n",
       " 'Cheyenne Lu\\nClass of 2027\\nClare Kim\\nClass of 2027',\n",
       " 'Class of 2027\\nClement Truong\\nClass of 2027',\n",
       " 'Class of 2027\\nCoco Baek\\nClass of 2026\\nConnie Gao',\n",
       " 'Connie Gao\\nClass of 2027\\nCynthia Du\\nClass of 2026',\n",
       " 'Class of 2026\\nDavid Oplatka\\nClass of 2026',\n",
       " 'Class of 2026\\nDefne Tanyildiz\\nClass of 2026',\n",
       " 'Class of 2026\\nDhwani Beesanahalli\\nClass of 2028',\n",
       " 'Class of 2028\\nDylan Winward\\nClass of 2026',\n",
       " 'Class of 2026\\nEmil Goubasarian\\nClass of 2025',\n",
       " 'Class of 2025\\nEmilio Dulay\\nClass of 2026\\nEmma Wu',\n",
       " 'Emma Wu\\nClass of 2027\\nEric Huang\\nClass of 2025',\n",
       " 'Class of 2025\\nGregor MacDonald\\nClass of 2026',\n",
       " 'Class of 2026\\nHarry Tong\\nClass of 2026',\n",
       " 'Class of 2026\\nIsaac Blender\\nClass of 2025',\n",
       " 'Class of 2025\\nJanet Yu\\nClass of 2026\\nJerome Teo',\n",
       " 'Jerome Teo\\nClass of 2026\\nJessica Guan',\n",
       " 'Jessica Guan\\nClass of 2027\\nJessup Byun',\n",
       " 'Jessup Byun\\nClass of 2027\\nJonah Jung',\n",
       " 'Jonah Jung\\nClass of 2025\\nJustin Nguyen',\n",
       " 'Justin Nguyen\\nClass of 2028\\nKaitlyn Baughman',\n",
       " 'Kaitlyn Baughman\\nClass of 2027\\nKarl Wang',\n",
       " 'Karl Wang\\nClass of 2027\\nKayla Hamakawa',\n",
       " 'Kayla Hamakawa\\nClass of 2027\\nKevin Hamakawa',\n",
       " 'Kevin Hamakawa\\nClass of 2025\\nKiara Cueva',\n",
       " 'Kiara Cueva\\nClass of 2026\\nKimberly Cui',\n",
       " 'Kimberly Cui\\nClass of 2027\\nLara Papasian',\n",
       " 'Lara Papasian\\nClass of 2025\\nLeah Shin',\n",
       " 'Leah Shin\\nClass of 2027\\nLeo Cardozo\\nClass of 2026',\n",
       " 'Class of 2026\\nLoretta Hu\\nClass of 2025',\n",
       " 'Class of 2025\\nMadelaine Leitman\\nClass of 2025',\n",
       " 'Class of 2025\\nMadelyn Yip\\nClass of 2027',\n",
       " 'Class of 2027\\nMaia Smolyanov\\nClass of 2025',\n",
       " 'Class of 2025\\nMegan Nguyen\\nClass of 2027',\n",
       " 'Class of 2027\\nMiles Garofola-Lam\\nClass of 2025',\n",
       " 'Class of 2025\\nMillie Huang\\nClass of 2026',\n",
       " 'Class of 2026\\nNathan Trux\\nClass of 2025',\n",
       " 'Class of 2025\\nNathanael Nam\\nClass of 2025',\n",
       " 'Class of 2025\\nNicole Yamachika\\nClass of 2025',\n",
       " 'Class of 2025\\nNiket Patel\\nClass of 2025\\nOm Phadke',\n",
       " 'Om Phadke\\nClass of 2028\\nParth Doshi\\nClass of 2025',\n",
       " 'Class of 2025\\nRia Kundu\\nClass of 2027',\n",
       " 'Class of 2027\\nRithwik Narendra\\nClass of 2026',\n",
       " 'Class of 2026\\nRiya Bhatla\\nClass of 2026\\nRoy Cheng',\n",
       " 'Roy Cheng\\nClass of 2027\\nRushil Shah\\nClass of 2027',\n",
       " 'Class of 2027\\nSamantha Alejandre\\nClass of 2025',\n",
       " 'Class of 2025\\nSamuel Perrott\\nClass of 2026',\n",
       " 'Class of 2026\\nSanskriti Jain\\nClass of 2028',\n",
       " 'Class of 2028\\nShan Kunzru\\nClass of 2026',\n",
       " 'Class of 2026\\nShravani Chiddarwar\\nClass of 2027',\n",
       " 'Class of 2027\\nSneha Agarwal\\nClass of 2027',\n",
       " 'Class of 2027\\nSong Chen\\nClass of 2026\\nSyed Islam',\n",
       " 'Syed Islam\\nClass of 2027\\nTanmay Desai',\n",
       " 'Tanmay Desai\\nClass of 2025\\nTony He\\nClass of 2026',\n",
       " 'Class of 2026\\nVivian Lee\\nClass of 2025\\nZahra Umar',\n",
       " 'Zahra Umar\\nClass of 2026',\n",
       " '© 2024 Data Science Union. All rights reserved.',\n",
       " 'Projects — UCLA Data Science Union\\nFeatured Internal Projects',\n",
       " 'Internal Projects are pitched by club members and focus on using various data science techniques to',\n",
       " 'data science techniques to creatively explore a topic of interest. Check out some of our previous',\n",
       " 'out some of our previous work below or on our medium or github.',\n",
       " 'Social Intelligence: Emotion Prediction in Videos\\n  \\n    News Generation',\n",
       " 'News Generation\\n  \\n    Text Summarization',\n",
       " 'Enhancing Spotify Playlists using their Audio Features with Classical and Deep Learning Methods',\n",
       " 'Predicting Plane Ticket Prices\\n  \\n    Healthy Communities',\n",
       " 'Healthy Communities\\n  \\n© 2024 Data Science Union. All rights reserved.',\n",
       " 'Contact — UCLA Data Science Union\\nContact Us',\n",
       " 'Contact Us\\nAre you a company looking to collaborate on a project with us? Are you a student with questions about what we do? Feel free to contact us and we will get back to you as soon as we can!',\n",
       " 'For general inquiries, our email is ucladsu@gmail.com.\\n              Name\\n              \\n            \\n              Email\\n              \\n            \\n              Subject',\n",
       " 'Message\\n              \\n            \\nThank you!\\n© 2024 Data Science Union. All rights reserved.',\n",
       " 'Apply — UCLA Data Science Union\\nRecruitment\\nOur application for the fall recruitment cycle has closed. We’ll be recruiting again Spring 2025 and look forward to reading your application!\\nFAQ\\n            What does DSU look for in their applicants?',\n",
       " 'We look for students who are passionate about developing their  skills in data science alongside like-minded peers and seek to answer questions through data-driven methods. We love seeing how you are personally inspired by data science, and we want to hear about how DSU fits into that inspiration!',\n",
       " 'What skills is DSU expecting? Do I need to take certain classes to be prepared?',\n",
       " 'We recommend that applicants have some prior experience with data analysis or statistical programming, but this is not strictly necessary. Many of our members have little to no experience with programming or data science when they join. Our proprietary two-part curriculum is designed to get all new',\n",
       " 'two-part curriculum is designed to get all new members up to speed and ready to contribute to projects!',\n",
       " 'How can I prepare for the interviews?\\n          \\nBe yourself, and bring your personality! Be ready to talk about who you are, your values, and your goals in both a group and individual setting!',\n",
       " 'Can I apply if I am an incoming freshman? Are there any major restrictions for applying?\\n          \\nWe welcome all students, regardless of their year or academic background. Membership is only available to current UCLA students at this time.',\n",
       " 'I have experience doing data-related projects. Can I skip curriculum and directly join the project teams?',\n",
       " 'If you’re interested in skipping the curriculum track, let us know during your interview! We’ll follow up with you to determine whether you’re a good fit to directly join project teams.\\n© 2024 Data Science Union. All rights reserved.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all of the splits into one list\n",
    "combined_splits = homepage_split + clients_split + about_split + team_split + projects_split + contact_split + apply_split\n",
    "combined_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd88c4a",
   "metadata": {},
   "source": [
    "Sweet! At this point, we have all of our website tabs split and saved and we're ready to move on to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ee482",
   "metadata": {},
   "source": [
    "# Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4c9ffc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/05/89/7eb147a37b7f31d3c815543df539d8b8d0425e93296c875cc87719d65232/sentence_transformers-3.4.1-py3-none-any.whl.metadata\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.41.0 from https://files.pythonhosted.org/packages/bd/40/902c95a2a6f5d2d120c940ac4bd1f937c01035af529803c13d65ca33c2d1/transformers-4.48.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m171.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for torch>=1.11.0 from https://files.pythonhosted.org/packages/0b/fa/f33a4148c6fb46ca2a3f8de39c24d473822d5774d652b66ed9b1214da5f7/torch-2.6.0-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.6.0-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.20.0 from https://files.pythonhosted.org/packages/ea/da/6c2bea5327b640920267d3bf2c9fc114cfbd0a5de234d81cda80cc9e33c8/huggingface_hub-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/e2/94/758680531a00d06e471ef649e4ec2ed6bf185356a7f9fbfbb7368a40bd49/fsspec-2025.2.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Obtaining dependency information for sympy==1.13.1 from https://files.pythonhosted.org/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/7a/88e58bb297c22633ed1c9d16029316e5b5ac5ee44012164c2edede599a5e/tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/24/84/e9d3ff57ae50dd0028f301c9ee064e5087fe8b00e55696677a0413c377a7/safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m372.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m231.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp311-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m702.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m920.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.9/408.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, safetensors, fsspec, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: sympy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.2\n",
      "    Uninstalling safetensors-0.3.2:\n",
      "      Successfully uninstalled safetensors-0.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.1\n",
      "    Uninstalling transformers-4.32.1:\n",
      "      Successfully uninstalled transformers-4.32.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2025.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2025.2.0 huggingface-hub-0.28.1 safetensors-0.5.2 sentence-transformers-3.4.1 sympy-1.13.1 tokenizers-0.21.0 torch-2.6.0 transformers-4.48.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fd379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70564c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "['UCLA Data Science Union\\nCultivating the next generation of data scientists\\n    Meet Our Team\\n  \\nWho We Are', 'The Data Science Union is a 501(c)(3) non-profit student-led organization founded in March 2019. Our goal is to cultivate a thriving community of tight-knit data science professionals by bringing', 'tight-knit data science professionals by bringing together those who share an interest in data science. The four components of our club — our self-designed curriculum, real-world projects,', 'self-designed curriculum, real-world projects, professional development opportunities, and focus on community — prepare members for futures in data science.', 'Learn More\\n  \\n70+\\nactive members\\n20+\\ninternal projects\\n10+\\nclient projects\\n15+\\ncompanies represented\\nWhere We Work\\n© 2024 Data Science Union. All rights reserved.']\n"
     ]
    }
   ],
   "source": [
    "print(type(combined_splits))\n",
    "print(type(combined_splits[0]))  # Check the type of the first element\n",
    "print(combined_splits[:5])  # Print a few elements to inspect them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea6b7403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00439329, -0.03681102, -0.01814641, ..., -0.02539857,\n",
       "       -0.04669847,  0.0110353 ], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = [model.encode(chunk) for chunk in combined_splits]\n",
    "embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef5b4f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With our proprietary data science curriculum, we train our members to become experts in modern data science technologies. We construct each team to provide clients with experts in software \n",
      "\n",
      "team to provide clients with experts in software engineering, machine learning, and data strategy. We have provided services in AI, analytics engineering, predictive modeling, software development, \n",
      "\n",
      "Dot product:  0.86411715\n"
     ]
    }
   ],
   "source": [
    "print(combined_splits[6], \"\\n\")\n",
    "print(combined_splits[7], \"\\n\")\n",
    "import numpy as np\n",
    "print(\"Dot product: \", np.dot(embeddings[6], embeddings[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11fd0142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DSU-relevant example: \n",
    "import numpy as np\n",
    "\n",
    "prompt_example = 'Who is currently the Membership director of Data Science Union?'\n",
    "prompt_embedding = model.encode(prompt_example)\n",
    "len(combined_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e647faa",
   "metadata": {},
   "source": [
    "# Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5cfe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /opt/anaconda3/lib/python3.12/site-packages (0.6.3)\r\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.2.2.post1)\r\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (2.8.2)\r\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.7.6)\r\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.115.8)\r\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\r\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.26.4)\r\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (3.13.0)\r\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (4.11.0)\r\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.20.1)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.30.0)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.30.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.51b0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.30.0)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.21.0)\r\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.48.9)\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (4.66.5)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (7.4.0)\r\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (6.5.2)\r\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.70.0)\r\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (4.2.1)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.9.0)\r\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (32.0.0)\r\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (8.2.3)\r\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (6.0.1)\r\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (5.1.0)\r\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (3.10.15)\r\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.27.0)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (13.7.1)\r\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/anaconda3/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (24.1)\r\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\r\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.45.3)\r\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\r\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.7)\r\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\r\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\r\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\r\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\r\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\r\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\r\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\r\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\r\n",
      "Requirement already satisfied: opentelemetry-util-http==0.51b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\r\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\r\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596c841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/0w7v1rzj34j4c7zm2s2lwzgm0000gn/T/ipykernel_46525/143103099.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49dcf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'Chatbot Project/website_info/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f713802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./Chatbot Project/website_info  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07c34324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document  # Import Document class\n",
    "\n",
    "# Ensure each entry in combined_splits is wrapped in a Document object\n",
    "combined_splits = [Document(page_content=text) if isinstance(text, str) else text for text in combined_splits]\n",
    "\n",
    "#vectordb = Chroma.from_documents(\n",
    "#    documents=combined_splits,\n",
    "#    embedding=embeddings,\n",
    "#    persist_directory=persist_directory\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd534ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an embedding function wrapper for ChromaDB\n",
    "class CustomEmbeddingFunction:\n",
    "    def embed_documents(self, texts):\n",
    "        return model.encode(texts, normalize_embeddings=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return model.encode([text], normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "embedding_function = CustomEmbeddingFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a85cc458",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=combined_splits,  # List of Document objects\n",
    "    embedding=embedding_function,  # Use the custom embedding function\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68520dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3717d91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Obtaining dependency information for chromadb from https://files.pythonhosted.org/packages/28/8e/5c186c77bf749b6fe0528385e507e463f1667543328d76fd00a49e1a4e6a/chromadb-0.6.3-py3-none-any.whl.metadata\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Obtaining dependency information for build>=1.0.3 from https://files.pythonhosted.org/packages/84/c2/80633736cd183ee4a62107413def345f7e6e3c01563dbca1417363cf957e/build-1.2.2.post1-py3-none-any.whl.metadata\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (2.10.6)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Obtaining dependency information for chroma-hnswlib==0.7.6 from https://files.pythonhosted.org/packages/0d/19/aa6f2139f1ff7ad23a690ebf2a511b2594ab359915d7979f76f3213e46c4/chroma_hnswlib-0.7.6-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Obtaining dependency information for fastapi>=0.95.2 from https://files.pythonhosted.org/packages/8f/7d/2d6ce181d7a5f51dedb8c06206cbf0ec026a99bf145edd309f9e17c3282f/fastapi-0.115.8-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Obtaining dependency information for uvicorn[standard]>=0.18.3 from https://files.pythonhosted.org/packages/61/14/33a3a1352cfa71812a3a21e8c9bfb83f60b0011f5e36f2b1399d51928209/uvicorn-0.34.0-py3-none-any.whl.metadata\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (1.24.3)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Obtaining dependency information for posthog>=2.4.0 from https://files.pythonhosted.org/packages/f7/d2/6d4330eff3df0aea78b6143945f51124465e1d4d4e02695ded6d066ab546/posthog-3.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading posthog-3.13.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Obtaining dependency information for onnxruntime>=1.14.1 from https://files.pythonhosted.org/packages/95/8d/2634e2959b34aa8a0037989f4229e9abcfa484e9c228f99633b3241768a6/onnxruntime-1.20.1-cp311-cp311-macosx_13_0_universal2.whl.metadata\n",
      "  Downloading onnxruntime-1.20.1-cp311-cp311-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-api>=1.2.0 from https://files.pythonhosted.org/packages/36/0a/eea862fae6413d8181b23acf8e13489c90a45f17986ee9cf4eab8a0b9ad9/opentelemetry_api-1.30.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-exporter-otlp-proto-grpc>=1.2.0 from https://files.pythonhosted.org/packages/5e/35/d9f63fd84c2ed8dbd407bcbb933db4ed6e1b08e7fbdaca080b9ac309b927/opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-instrumentation-fastapi>=0.41b0 from https://files.pythonhosted.org/packages/55/1c/ec2d816b78edf2404d7b3df6d09eefb690b70bfd191b7da06f76634f1bdc/opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Obtaining dependency information for opentelemetry-sdk>=1.2.0 from https://files.pythonhosted.org/packages/97/28/64d781d6adc6bda2260067ce2902bd030cf45aec657e02e28c5b4480b976/opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (0.21.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m518.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Obtaining dependency information for overrides>=7.3.1 from https://files.pythonhosted.org/packages/2c/ab/fc8290c6a4c722e5514d80f62b2dc4c4df1a68a41d1364e625c35990fcf3/overrides-7.7.0-py3-none-any.whl.metadata\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Obtaining dependency information for importlib-resources from https://files.pythonhosted.org/packages/a4/ed/1f1afb2e9e7f38a545d628f864d562a5ae64fe6f7a10e28ffb9b185b4e89/importlib_resources-6.5.2-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Obtaining dependency information for grpcio>=1.58.0 from https://files.pythonhosted.org/packages/e4/bd/cc36811c582d663a740fb45edf9f99ddbd99a10b6ba38267dc925e1e193a/grpcio-1.70.0-cp311-cp311-macosx_10_14_universal2.whl.metadata\n",
      "  Downloading grpcio-1.70.0-cp311-cp311-macosx_10_14_universal2.whl.metadata (3.9 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Obtaining dependency information for bcrypt>=4.0.1 from https://files.pythonhosted.org/packages/4a/57/23b46933206daf5384b5397d9878746d2249fe9d45efaa8e1467c87d3048/bcrypt-4.2.1-cp39-abi3-macosx_10_12_universal2.whl.metadata\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-macosx_10_12_universal2.whl.metadata (9.8 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Obtaining dependency information for typer>=0.9.0 from https://files.pythonhosted.org/packages/d0/cc/0a838ba5ca64dc832aa43f727bd586309846b0ffb2ce52422543e6075e8a/typer-0.15.1-py3-none-any.whl.metadata\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Obtaining dependency information for kubernetes>=28.1.0 from https://files.pythonhosted.org/packages/df/14/a59acfe4b3095f2a4fd8d13b348853a69c8f1ed4bce9af00d1b31351a88e/kubernetes-32.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading kubernetes-32.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Obtaining dependency information for tenacity>=8.2.3 from https://files.pythonhosted.org/packages/b6/cb/b86984bed139586d01532a587464b5805f12e397594f19f931c4c2fbfa61/tenacity-9.0.0-py3-none-any.whl.metadata\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (6.0)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Obtaining dependency information for mmh3>=4.0.1 from https://files.pythonhosted.org/packages/4f/21/25ea58ca4a652bdc83d1528bec31745cce35802381fb4fe3c097905462d2/mmh3-5.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Obtaining dependency information for rich>=10.11.0 from https://files.pythonhosted.org/packages/19/71/39c7c0d87f8d4e6c020a393182060eaefeeae6c01dab6a84ec346f2567df/rich-13.9.4-py3-none-any.whl.metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Obtaining dependency information for pyproject_hooks from https://files.pythonhosted.org/packages/bd/24/12818598c362d7f300f18e74db45963dbcb85150324092410c8b49405e42/pyproject_hooks-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Obtaining dependency information for starlette<0.46.0,>=0.40.0 from https://files.pythonhosted.org/packages/d9/61/f2b52e107b1fc8944b33ef56bf6ac4ebbe16d91b94d2b87ce013bf63fb84/starlette-0.45.3-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: anyio in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.5.0)\n",
      "Requirement already satisfied: certifi in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for google-auth>=1.0.1 from https://files.pythonhosted.org/packages/9d/47/603554949a37bca5b7f894d51896a9c534b9eab808e2520a748e081669d0/google_auth-2.38.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.31.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for requests-oauthlib from https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for oauthlib>=3.2.2 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.16)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for durationpy>=0.7 from https://files.pythonhosted.org/packages/4c/a3/ac312faeceffd2d8f86bc6dcb5c401188ba5a01bc88e69bed97578a0dfcd/durationpy-0.9-py3-none-any.whl.metadata\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for coloredlogs from https://files.pythonhosted.org/packages/a7/06/3d6badcf13db419e25b07041d9c7b4a2c331d3f4e7134445ec5df57714cd/coloredlogs-15.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for flatbuffers from https://files.pythonhosted.org/packages/b8/25/155f9f080d5e4bc0082edfda032ea2bc2b8fab3f4d25d46c1e9dd22a1a89/flatbuffers-25.2.10-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/dd/04/3eaedc2ba17a088961d0e3bd396eac764450f431621b58a04ce898acd126/protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: sympy in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for deprecated>=1.2.6 from https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl.metadata\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.0.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for googleapis-common-protos~=1.52 from https://files.pythonhosted.org/packages/89/30/2bd0eb03a7dee7727cd2ec643d1e992979e62d5e7443507381cce0455132/googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-exporter-otlp-proto-common==1.30.0 from https://files.pythonhosted.org/packages/ee/54/f4b3de49f8d7d3a78fd6e6e1a6fd27dd342eb4d82c088b9078c6a32c3808/opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-proto==1.30.0 from https://files.pythonhosted.org/packages/56/d7/85de6501f7216995295f7ec11e470142e6a6e080baacec1753bbf272e007/opentelemetry_proto-1.30.0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-instrumentation-asgi==0.51b0 from https://files.pythonhosted.org/packages/54/7e/0a95ab37302729543631a789ba8e71dea75c520495739dbbbdfdc580b401/opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-instrumentation==0.51b0 from https://files.pythonhosted.org/packages/40/2c/48fa93f1acca9f79a06da0df7bfe916632ecc7fce1971067b3e46bcae55b/opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-semantic-conventions==0.51b0 from https://files.pythonhosted.org/packages/2e/75/d7bdbb6fd8630b4cafb883482b75c4fc276b6426619539d266e32ac53266/opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for opentelemetry-util-http==0.51b0 from https://files.pythonhosted.org/packages/48/dd/c371eeb9cc78abbdad231a27ce1a196a37ef96328d876ccbb381dea4c8ee/opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata\n",
      "  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Obtaining dependency information for asgiref~=3.0 from https://files.pythonhosted.org/packages/39/e3/893e8757be2612e6c266d9bb58ad2e3651524b5b40cf56761e985a28b13e/asgiref-3.8.1-py3-none-any.whl.metadata\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Obtaining dependency information for monotonic>=1.5 from https://files.pythonhosted.org/packages/9a/67/7e8406a29b6c45be7af7740456f7f37025f0506ae2e05fb9009a53946860/monotonic-1.6-py2.py3-none-any.whl.metadata\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Obtaining dependency information for backoff>=1.10.0 from https://files.pythonhosted.org/packages/df/73/b6e24bd22e6720ca8ee9a85a0c4a2971af8497d8f3193fa05390cbd46e09/backoff-2.2.1-py3-none-any.whl.metadata\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Obtaining dependency information for shellingham>=1.3.0 from https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for httptools>=0.6.3 from https://files.pythonhosted.org/packages/a6/17/3e0d3e9b901c732987a45f4f94d4e2c62b89a041d93db89eafb262afd8d5/httptools-0.6.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading httptools-0.6.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for uvloop!=0.15.0,!=0.15.1,>=0.14.0 from https://files.pythonhosted.org/packages/57/a7/4cf0334105c1160dd6819f3297f8700fda7fc30ab4f61fbf3e725acbc7cc/uvloop-0.21.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for watchfiles>=0.13 from https://files.pythonhosted.org/packages/55/88/9ebf36b3547176d1709c320de78c1fa3263a46be31b5b1267571d9102686/watchfiles-1.0.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading watchfiles-1.0.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Obtaining dependency information for websockets>=10.4 from https://files.pythonhosted.org/packages/a1/c6/1435ad6f6dcbff80bb95e8986704c3174da8866ddb751184046f5c139ef6/websockets-14.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading websockets-14.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/ec/4e/de4ff18bcf55857ba18d3a4bd48c8a9fde6bb0980c9d20b263f05387fd88/cachetools-5.5.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from requests->kubernetes>=28.1.0->chromadb) (2.0.4)\n",
      "Collecting anyio (from httpx>=0.27.0->chromadb)\n",
      "  Obtaining dependency information for anyio from https://files.pythonhosted.org/packages/46/eb/e7f063ad1fec6b3178a3cd82d1a3c4de82cccf283fc42746168188e1cdd5/anyio-4.8.0-py3-none-any.whl.metadata\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Obtaining dependency information for humanfriendly>=9.1 from https://files.pythonhosted.org/packages/f0/0f/310fb31e39e2d734ccaa2c0fb981ee41f7bd5056ce9bc29b2248bd569169/humanfriendly-10.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/charliehoose/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-macosx_11_0_arm64.whl (185 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-macosx_10_12_universal2.whl (489 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 kB\u001b[0m \u001b[31m177.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m469.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.70.0-cp311-cp311-macosx_10_14_universal2.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading kubernetes-32.0.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m505.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-macosx_11_0_arm64.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-macosx_13_0_universal2.whl (31.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m435.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.13.0-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.8/210.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-macosx_11_0_arm64.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.8/417.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m267.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m788.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m517.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-macosx_11_0_arm64.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-14.2-cp311-cp311-macosx_11_0_arm64.whl (160 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.0/161.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53771 sha256=0b75ef08d2c2c41338bdf0b9ceb3f9cb5d0c2e681ef9b10c1a0920c924bdbb7e\n",
      "  Stored in directory: /Users/charliehoose/Library/Caches/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, websockets, uvloop, uvicorn, tenacity, shellingham, rsa, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, oauthlib, mmh3, importlib-resources, humanfriendly, httptools, grpcio, deprecated, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, anyio, watchfiles, starlette, rich, requests-oauthlib, posthog, opentelemetry-proto, opentelemetry-api, googleapis-common-protos, google-auth, coloredlogs, build, typer, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.5.0\n",
      "    Uninstalling anyio-3.5.0:\n",
      "      Successfully uninstalled anyio-3.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anyio-4.8.0 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 cachetools-5.5.1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 deprecated-1.2.18 durationpy-0.9 fastapi-0.115.8 flatbuffers-25.2.10 google-auth-2.38.0 googleapis-common-protos-1.67.0 grpcio-1.70.0 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-32.0.0 mmh3-5.1.0 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-instrumentation-fastapi-0.51b0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 overrides-7.7.0 posthog-3.13.0 protobuf-5.29.3 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 rich-13.9.4 rsa-4.9 shellingham-1.5.4 starlette-0.45.3 tenacity-9.0.0 typer-0.15.1 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4 websockets-14.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa2b6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f470d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"who is the membership director?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a58e8e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Charlie Hoose\\nDirector of Membership')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b34d4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what does DSU look for in its applicants?\"\n",
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7325598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='We look for students who are passionate about developing their  skills in data science alongside like-minded peers and seek to answer questions through data-driven methods. We love seeing how you are personally inspired by data science, and we want to hear about how DSU fits into that inspiration!')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc124269",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how does DSU promote professional development?\"\n",
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "662f1349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='In addition to help members grow professionally, we also place a large emphasis on personal growth. At DSU, we seek to cultivate an environment in which members can form meaningful and long-lasting')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ed881",
   "metadata": {},
   "source": [
    "## Retrieval QA and Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdae500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'REDACTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8363b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm_name = \"gpt-3.5-turbo\" # Can used more advanced model, but for our this should be sufficient\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key = openai.api_key)\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24fd934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is creating a chain using an inputted LLM and the vector database (Refer to image on slides)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9630802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9dd7dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What color is grass?\n",
      "\n",
      "Response: page_content='We look for students who are passionate about developing their  skills in data science alongside like-minded peers and seek to answer questions through data-driven methods. We love seeing how you are personally inspired by data science, and we want to hear about how DSU fits into that inspiration!'\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question)\n",
    "print()\n",
    "print(\"Response:\", docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a764e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say da fuq you askin me for, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "Context: {context}\n",
    "Question: {question}\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a04dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are DSU's core pillars?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0cd2f0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DSU's core pillars include professional growth and personal development. They aim to help members grow both professionally and personally, creating an environment for meaningful and long-lasting relationships.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
